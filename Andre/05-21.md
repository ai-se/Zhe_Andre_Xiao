# 05-21-2020

# Contact List Requested:

* https://docs.google.com/spreadsheets/d/1D0GLUH0rNXpQYGfmb3AvOTWHna6kv4ccIo0oegeHvVM/edit#gid=0

# Questions


### Costs associated with increasing the number of speech acts to classify:
* Annotation Cost - With more classes its harder to annotate the same quantity of data.
* Bigger Ontology necessary (depending on end technology adopted)
* More expert knowledge needed (depending on end technology adopted)

### Comparison of advantages between simple vs complicated:

Simple cases are usually easy to annotate and “Solve" (independent of technology adopted)

There is also the possibility of manually developing rules to classify speech acts which would give us good accuracy and recall when the problem at hand is “Simple enough”  

Complicated gives us a broader horizon, obtaining information we didn’t even know we wanted at first. However it seems that simplifying the classes and specializing them to the domain at hand is the general consensus of the literature as the way to go.

# Generic Workflow for Speech Act Classification



![General Workflow for Speech Act Classification](https://raw.githubusercontent.com/ai-se/Zhe_Andre_Xiao/master/Andre/img/SpeechActFlow.png)

# New Questions


1. Why is the speech act area so underexplored in SE and in general?

  

2. Given that speech acts are an intermediate step for some form of end goal prediction (Similar to POS tagging) What should be our story? As in what should we look into solve in SE using speech acts.

  

# SE Application vs General

  

The few papers that use speech acts in SE have no learning involved, The rules for classification of the speech acts are all manually generated by experts and they use something called the GATE Framework. In the broader research in other domains (Not SE) there is a plethora of papers using many different methods. But one of the things that stands out is the fact that the results are usually on the lower end, and given that Speech acts is by definition an unbalanced data domain (Most speech acts have to do with expressions and statements), there must be something that can be done with those papers to improve the results on these unbalanced data sets.

  

## Domain Quirks

* Data is usually unbalanced - If we go from the basic 4 classes of speech acts:
	* Constantives - Statements of beliefs, intentions or desires 
	* Directives - Attitude towards some prospective action to be performed by the hearer 
	* Commissives - Intention to commit to do something 
	* Acknowledgements - Feelings regarding the hearer or the speaker’s intention that the utterance satisfies a social expectation
* With these 4 basic classes most of the sentences are in the **constantive** class, the second most common being **directives** with **commissives** and **acknowledgements** being rare. (This in the case of monologues, which will differ from dialogues)
* I have yet to find a paper that uses **surrounding** speech act context to predict the next speech act. 

