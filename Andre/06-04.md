# 06-04-2020



# Am I On the right path?


### Yes and No
In general, the usage of the usual nlp tools can use Pre-existing expert knowledge or not. By reading papers on automatic requirements extraction. There is a good amount of papers that use Expert Knowledge and that dont.

However while reading these papers one thing became clear. The point of speech acts is to detect subtext in text. And that is recognized as usually a difficult thing to do in NLP. 

So for the specific case of automatic requirement detection /classification in text. The answer is No I am not in the right path. There is a plethora of papers that simply use automatic feature selection or a dictionary (automatically generated) of the most common terms in order to classify and automatically generate rules (Rule mining). Or even simply using LDA and tf-idf to classify the text.

However is there a problem where detecting subtext would be important and the usage of NLP needs the expert knowledge?

The thing is these automatic approaches without a previous knowledge base are in general using counts or statistics on the presence or absence of words in text, excluding self-learning onthologies and pre-built knowledge. When we are talking about the linguistic domain and the usage of ML to understand text there is much we don't know. The state of the art for language processing is nowhere near image processing for example. And one of the things that may cause this is that language is more than the words in the text (eg.: Speech Acts, Intent, Effect) these are all parts of the semantic understanding of the text. 

One might ask then **How to even learn subtext** or **How to do semantic learning**. With the tools I've read so far I don't see a way to do so. There might be a way towards that on self learning onthologies. But I'm not up to date on how those work and didn't want to go chasing another rabbit hole.

So I guess the question moving forward is. If we are to keep at requirement discovery as a task. In what way can we be different from others and do it better/faster/cheaper? If we are not to keep at requirement discovery as a task. Which task could be better by applying automatic rule learning and feature extraction on text? And does that task draw a parallel to the software engineering domain?

## Tell me about it

During the reading I came upon this very idea of developing a "Tell me about it" Engine by using text summarization and subtext altogether. Instead of it being a tool for a final purpose, this tool would allow users to get the general intentions and concepts of the text. This could be applied to a multitude of things. From academic papers, to software engineering documents, to news articles.

The goal would be to display visually to the user the breakdown information on the text presented on screen. Automatically detecting the intentions of the text (Speech Act), the thematic (LDA), and the general path of the text (using Knowledge-Graphs). This would create a solution that simulates human-like understanding of text and summarize the text to the final user.

This could be an improvement for example on Zhe's fast read method, where if we not only had the abstract but the papers themselves we could process them and display on screen the relevant information about that article so that the human can make better decisions, on wether a paper is or isn't relevant. The information gathered here could also feed the SVM and make better decisions on the boundaries.
