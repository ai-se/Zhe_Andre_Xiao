# 22-04

## Read some papers on 

* https://scholar.google.com/scholar?cites=11010542120175601159&as_sdt=5,34&sciodt=0,34&hl=en

### Takeaways

The purpose of the research line that came from this paper is very different from what we are aiming to achieve 
**(Peak into the future with speech acts)**
, but nonetheless some results are interesting.


The work is to improve the POS Tagging for actual code recognition, from there they try to extract the different 
mentions to known API's, consolidate a base of semantically linkable knowledge and predict it

For example in https://chunyang-chen.github.io/publication/analogyJournal_EMSE.pdf

The goal is to in SO Questions extract analogical libraries and APIs from one language to another, or even within the same language.

This is done by extracting the API mentions from the questions by using POS tagging, word embedding, and association rule learning
to extract API mentions and compare the correlation of mention and usage of the API in different questions.

The search for the analogical APIs is made in a vector space of tagged APIs, where say for example we have the words python and nltk
we can look for a similar api in java (openlp) using  K-nearest-neighbor search for the tags whose word representation
is the most similar to the vector nltk − python + java

Another example in: https://dl.acm.org/doi/pdf/10.1145/2970276.2970357

The goal is to predict wether two questions in SO are into one of these 4 classes:

* **Duplicate**: Two knowledge units discuss the same ques-tion in diﬀerent ways, and can be answered by the sameanswer.
* **Direct link**: One knowledge unit can help solve theproblem in the other knowledge unit, for example, byexplaining certain concepts, providing examples, orcovering a sub-step for solving a complex problem.
* **Indirect link**: One knowledge unit provides related in-formation, but it does not directly answer the questionin the other knowledge unit.
* **Isolated**: The two knowledge units are not semantically related

This is done using a convolutional neural network **(maybe we could try to beat those results without using a deep learning approach, and using my approach of optimizing embeddings)** into:

* a previous database with the information of the classes **(User-created links that are considered ground truth)**
* word2vec embeddings on the text with a skip-gram model.

They also compare it to:

* Multiclass SVM Classifier using TF-IDF
* Multiclass SVM Classifier using Word Embeddings

The results were:

| Method   |      Composition      |  Accuracy |
|----------|-------------|------|
| Baseline1 |  TF-IDF + SVM | **0.625**  |
| Baseline2 |    Embedding + SVM   |   **0.669** |
| Their Approach | Word Embedding + CNN |  **0.841** |





